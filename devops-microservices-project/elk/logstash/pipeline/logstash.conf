input {
  beats {
    port => 5044
    host => "0.0.0.0"
  }
  
  # file {
  #   path => "/user_logs/*.log"
  #   start_position => "beginning"
  #   sincedb_path => "/dev/null"
  #   codec => "json"
  # }
  
  # file {
  #   path => "/product_logs/*.log"
  #   start_position => "beginning"
  #   sincedb_path => "/dev/null"
  #   codec => "json"
  # }
}

filter {
  # Parse Docker logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      skip_on_invalid_json => true
    }
  }
  
  # Add service identification
  # if "user_service" in [container] {
  #   mutate {
  #     add_field => { "service" => "user_service" }
  #   }
  # }
  
  # if "product_service" in [container] {
  #   mutate {
  #     add_field => { "service" => "product_service" }
  #   }
  # }

  if [log] and [log][file] and "user" in [log][file][path] {
    mutate {
      add_field => { "service" => "user_service" }
    }
  }
  
  if [log] and [log][file] and "product" in [log][file][path] {
    mutate {
      add_field => { "service" => "product_service" }
    }
  }
  
  # Parse timestamp
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }
  
  # Parse log levels
  # grok {
  #   match => { "message" => "%{LOGLEVEL:log_level}" }
  # }
  
  # Remove unnecessary fields
  mutate {
    remove_field => [ "agent", "ecs", "host", "input", "log", "tags" ]
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "microservices-logs-%{+YYYY.MM.dd}"
    # flush_size => 500
    # idle_flush_time => 5
  }
  
  # For debugging
  stdout { codec => rubydebug }
}